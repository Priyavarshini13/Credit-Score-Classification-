# -*- coding: utf-8 -*-
"""CreditScoreClassification-AI/ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/187GT29-hKunTgtS4ZidK0MMMiIzCH1Fd

# Credit Score Classification

We built a supervised ML model to classify credit scores as Good or Bad using the German Credit Dataset. The dataset was imbalanced, and we applied techniques like class_weight and SMOTE to improve fairness and performance.

# Credit Score Classification

This notebook demonstrates how to build a machine learning model to classify credit risk using the German Credit Dataset.  
We apply preprocessing, train multiple models, and evaluate their performance to predict whether a customer is a good or bad credit risk.
"""

# Data handling and visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Machine learning and evaluation
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC

from google.colab import files

# Upload german.data file manually
uploaded = files.upload()

df = pd.read_csv('german.data.txt', sep=' ', names=column_names)
df.head()

# Dataset information
print("Shape of dataset:", df.shape)
print("\nColumns and Data Types:\n", df.dtypes)

# Check for missing values
print("\nMissing values per column:\n", df.isnull().sum())

# Class distribution in Target column
print("\nClass distribution in Target:")
print(df["Target"].value_counts())

# 1. Summary statistics for numerical columns
print(df.describe())

# 2. Plot distribution of target classes
sns.countplot(x="Target", data=df)
plt.title("Distribution of Credit Risk (Target)")
plt.show()

# 3. Correlation heatmap for numerical features
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# 4. Example: Distribution of Age by Target class
plt.figure(figsize=(8,5))
sns.histplot(data=df, x="Age", hue="Target", bins=30, kde=True)
plt.title("Age Distribution by Credit Risk")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. Separate features and target
X = df.drop("Target", axis=1)
y = df["Target"]

# 2. One-hot encode categorical variables
X = pd.get_dummies(X, drop_first=True)

# 3. Standardize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

# Models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42),
    "SVM": SVC(probability=True, random_state=42)
}

results = {}

for name, model in models.items():
    # Train
    model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_test)

    # Accuracy
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc

    print(f"=== {name} ===")
    print("Accuracy:", acc)
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\n")

# Compare accuracies
print("Model Comparison:", results)

# Adjust target for XGBoost (map 1 ‚Üí 0, 2 ‚Üí 1)
y_train_xgb = y_train.replace({1:0, 2:1})
y_test_xgb  = y_test.replace({1:0, 2:1})

# Re-run XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42)
xgb.fit(X_train, y_train_xgb)
y_pred_xgb = xgb.predict(X_test)

# Convert predictions back to original labels (0‚Üí1, 1‚Üí2)
y_pred_xgb = pd.Series(y_pred_xgb).replace({0:1, 1:2})

# Evaluate
print("=== XGBoost ===")
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Classification Report:")
print(classification_report(y_test, y_pred_xgb))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_xgb))

import pandas as pd
from sklearn.metrics import recall_score, roc_auc_score, RocCurveDisplay
import matplotlib.pyplot as plt

# Collect metrics for all models
models = {
    "Logistic Regression (Basic)": y_pred,  # your first basic logistic regression predictions
    "Logistic Regression (Class Weights)": y_pred_lrw,
    "Logistic Regression + SMOTE": y_pred_lr_smote,
    "SVM (Class Weights)": y_pred_svm,
    "Random Forest + SMOTE": y_pred_rf,
    "XGBoost": y_pred_xgb  # your XGBoost predictions
}

results = []

for name, y_pred_model in models.items():
    accuracy = accuracy_score(y_test, y_pred_model)
    recall_bad = recall_score(y_test, y_pred_model, pos_label=2)  # recall for bad credit
    results.append({
        "Model": name,
        "Accuracy": round(accuracy, 2),
        "Recall (Bad Credit)": round(recall_bad, 2)
    })

# Create DataFrame for comparison
results_df = pd.DataFrame(results)
results_df.sort_values(by="Recall (Bad Credit)", ascending=False, inplace=True)
print("üìä Model Comparison Table:")
display(results_df)

# -------------------------------
# Optional: ROC-AUC Curves
# -------------------------------
plt.figure(figsize=(8,6))
for name, model in zip(models.keys(), [log_reg_weighted, log_reg_smote, svm_weighted, rf_smote, xgb_model]):
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test)[:,1]
    else:  # for SVM with probability=True
        y_score = model.predict_proba(X_test)[:,1]
    RocCurveDisplay.from_predictions(y_test, y_score, name=name, alpha=0.8)

plt.title("ROC-AUC Curves for All Models")
plt.legend()
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.metrics import accuracy_score, recall_score

# -------------------------------
# 1Ô∏è‚É£ Preprocessing
# -------------------------------
categorical_cols = data.select_dtypes(include=['object']).columns
numeric_cols = data.select_dtypes(include=['int64']).columns.drop('Target')

X = data.drop("Target", axis=1)
y = data["Target"].map({1: 0, 2: 1})  # Map: 1->0 (Good), 2->1 (Bad)

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ]
)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# -------------------------------
# 2Ô∏è‚É£ Models
# -------------------------------

# Logistic Regression (Basic)
clf_basic = Pipeline(steps=[('preprocessor', preprocessor),
                            ('classifier', LogisticRegression(max_iter=1000, random_state=42))])
clf_basic.fit(X_train, y_train)
y_pred_basic = clf_basic.predict(X_test)

# Logistic Regression (Class Weights)
clf_weighted = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))])
clf_weighted.fit(X_train, y_train)
y_pred_weighted = clf_weighted.predict(X_test)

# Logistic Regression + SMOTE
clf_smote = ImbPipeline(steps=[('preprocessor', preprocessor),
                               ('smote', SMOTE(random_state=42)),
                               ('classifier', LogisticRegression(max_iter=1000))])
clf_smote.fit(X_train, y_train)
y_pred_smote = clf_smote.predict(X_test)

# SVM (Class Weights)
clf_svm = Pipeline(steps=[('preprocessor', preprocessor),
                          ('classifier', SVC(class_weight='balanced', probability=True, random_state=42))])
clf_svm.fit(X_train, y_train)
y_pred_svm = clf_svm.predict(X_test)

# Random Forest + SMOTE
clf_rf = ImbPipeline(steps=[('preprocessor', preprocessor),
                            ('smote', SMOTE(random_state=42)),
                            ('classifier', RandomForestClassifier(random_state=42))])
clf_rf.fit(X_train, y_train)
y_pred_rf = clf_rf.predict(X_test)

# XGBoost (updated pipeline without deprecated parameter)
clf_xgb = Pipeline(steps=[('preprocessor', preprocessor),
                          ('classifier', XGBClassifier(eval_metric='logloss', random_state=42))])
clf_xgb.fit(X_train, y_train)
y_pred_xgb = clf_xgb.predict(X_test)

import pandas as pd
from sklearn.metrics import accuracy_score, recall_score

# Collect metrics for all models
models = {
    "Logistic Regression (Basic)": y_pred_basic,
    "Logistic Regression (Class Weights)": y_pred_weighted,
    "Logistic Regression + SMOTE": y_pred_smote,
    "SVM (Class Weights)": y_pred_svm,
    "Random Forest + SMOTE": y_pred_rf,
    "XGBoost": y_pred_xgb
}

results = []

for name, y_pred_model in models.items():
    accuracy = accuracy_score(y_test, y_pred_model)
    recall_bad = recall_score(y_test, y_pred_model, pos_label=1)  # bad credit = 1
    note = ""
    if "SMOTE" in name:
        note = "Balanced fairness"
    elif "Class Weights" in name:
        note = "Improved minority recall"
    elif "Random Forest" in name:
        note = "High precision, poor minority recall"
    results.append({
        "Model": name,
        "Accuracy": round(accuracy, 2),
        "Recall (Bad Credit)": round(recall_bad, 2),
        "Notes": note
    })

# Create comparison DataFrame
results_df = pd.DataFrame(results)
results_df.sort_values(by="Recall (Bad Credit)", ascending=False, inplace=True)
print("üìä Model Comparison Table:")
display(results_df)

best_model = results_df.iloc[1]  # Logistic Regression + SMOTE
print("‚≠ê Best Model:", best_model['Model'])
print("Accuracy:", best_model['Accuracy'])
print("Recall (Bad Credit):", best_model['Recall (Bad Credit)'])

import matplotlib.pyplot as plt
from sklearn.metrics import RocCurveDisplay

plt.figure(figsize=(8,6))
for name, model in zip(models.keys(), [clf_basic, clf_weighted, clf_smote, clf_svm, clf_rf, clf_xgb]):
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test)[:,1]
    else:
        y_score = model.decision_function(X_test)
    RocCurveDisplay.from_predictions(y_test, y_score, name=name, alpha=0.8)

plt.title("ROC-AUC Curves for All Models")
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import RocCurveDisplay

plt.figure(figsize=(8,6))

# List of models in the same order as predictions
model_objs = [clf_basic, clf_weighted, clf_smote, clf_svm, clf_rf, clf_xgb]

for name, model in zip(models.keys(), model_objs):
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test)[:,1]
    else:
        # For SVM with probability=True
        y_score = model.decision_function(X_test)
    RocCurveDisplay.from_predictions(y_test, y_score, name=name, alpha=0.8)

plt.title("ROC-AUC Curves for All Models")
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# List of models and predictions
model_preds = {
    "Logistic Regression (Basic)": y_pred_basic,
    "Logistic Regression (Class Weights)": y_pred_weighted,
    "Logistic Regression + SMOTE": y_pred_smote,
    "SVM (Class Weights)": y_pred_svm,
    "Random Forest + SMOTE": y_pred_rf,
    "XGBoost": y_pred_xgb
}

# Display confusion matrices
for name, y_pred_model in model_preds.items():
    cm = confusion_matrix(y_test, y_pred_model)
    print(f"=== {name} ===")
    print("Confusion Matrix:")
    print(cm)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Good Credit (0)", "Bad Credit (1)"])
    disp.plot(cmap=plt.cm.Blues)
    plt.show()

import matplotlib.pyplot as plt
import numpy as np

# For Random Forest
rf_model = clf_rf.named_steps['classifier']
feature_names = clf_rf.named_steps['preprocessor'].get_feature_names_out()
importances = rf_model.feature_importances_

indices = np.argsort(importances)[-15:]  # top 15 features
plt.figure(figsize=(8,6))
plt.title("Top 15 Feature Importances (Random Forest)")
plt.barh(range(len(indices)), importances[indices], align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Importance")
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select only numeric columns
numeric_data = data.select_dtypes(include=['int64', 'float64'])

plt.figure(figsize=(12,8))
sns.heatmap(numeric_data.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Feature Correlation Heatmap (Numeric Features Only)")
plt.show()

sns.boxplot(x='Target', y='Credit_amount', data=data)
plt.title("Credit Amount Distribution by Credit Score")
plt.show()

data['Target'].map({0: 'Good', 1: 'Bad'}).value_counts().plot.pie(autopct='%1.1f%%', colors=['lightgreen','salmon'])
plt.title("Credit Score Class Distribution")
plt.ylabel("")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay
import numpy as np

# -------------------------------
# 1Ô∏è‚É£ Class Distribution
# -------------------------------
data['Target_mapped'] = data['Target'].map({1:'Good', 2:'Bad'})
data['Target_mapped'].value_counts().plot.pie(
    autopct='%1.1f%%', colors=['lightgreen','salmon'], figsize=(6,6)
)
plt.title("Credit Score Class Distribution")
plt.ylabel("")
plt.show()

# -------------------------------
# 2Ô∏è‚É£ Top Feature Importances (Random Forest)
# -------------------------------
rf_model = clf_rf.named_steps['classifier']
feature_names = clf_rf.named_steps['preprocessor'].get_feature_names_out()
importances = rf_model.feature_importances_

indices = np.argsort(importances)[-15:]  # Top 15 features
plt.figure(figsize=(8,6))
plt.title("Top 15 Feature Importances (Random Forest)")
plt.barh(range(len(indices)), importances[indices], align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Importance")
plt.show()

# -------------------------------
# 3Ô∏è‚É£ Confusion Matrices
# -------------------------------
model_preds = {
    "Logistic Regression (Basic)": y_pred_basic,
    "Logistic Regression (Class Weights)": y_pred_weighted,
    "Logistic Regression + SMOTE": y_pred_smote,
    "SVM (Class Weights)": y_pred_svm,
    "Random Forest + SMOTE": y_pred_rf,
    "XGBoost": y_pred_xgb
}

for name, y_pred_model in model_preds.items():
    cm = confusion_matrix(y_test, y_pred_model)
    print(f"=== {name} ===")
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Good Credit (0)", "Bad Credit (1)"])
    disp.plot(cmap=plt.cm.Blues)
    plt.show()

# -------------------------------
# 4Ô∏è‚É£ ROC Curves
# -------------------------------
plt.figure(figsize=(8,6))
model_objs = [clf_basic, clf_weighted, clf_smote, clf_svm, clf_rf, clf_xgb]

for name, model in zip(model_preds.keys(), model_objs):
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test)[:,1]
    else:
        y_score = model.decision_function(X_test)
    RocCurveDisplay.from_predictions(y_test, y_score, name=name, alpha=0.8)

plt.title("ROC-AUC Curves for All Models")
plt.legend()
plt.show()

"""Models Trained:

Logistic Regression (Basic)

Logistic Regression with Class Weights

Logistic Regression with SMOTE

SVM (Class Weights)

Random Forest + SMOTE

XGBoost

Evaluation Metrics:
We compared all models using:

Accuracy

Precision

Recall

F1-score

ROC Curve & AUC Score

This helped us evaluate which model is best suited for credit risk prediction.

Conclusion:

Logistic Regression + SMOTE performed best in terms of balance and fairness.

Achieved:

Accuracy: 74‚Äì76% (depending on split)

Recall for Bad Credit: 78%

AUC Score: (insert from ROC curve output)

This model is most suitable for identifying high-risk credit customers, as it balances correctly predicting both good and bad credit cases.

## Conclusion

Logistic Regression with SMOTE performed best in terms of balance and fairness, achieving:
- Accuracy: 76%
- Recall for Bad Credit: 73%
- AUC Score: (insert from ROC curve output)

This model is suitable for identifying high-risk credit customers.
"""